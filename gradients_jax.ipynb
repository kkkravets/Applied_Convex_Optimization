{"cells":[{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp"],"metadata":{"id":"E-osdHJzGGOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from jax.config import config\n","config.update(\"jax_enable_x64\", True)"],"metadata":{"id":"lkviP-zeGG8n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Problem 1"],"metadata":{"id":"cXW5HJnQ1GIa"}},{"cell_type":"markdown","source":["Compute the gradients with respect to $U \\in R^{n×k}$ and $V \\in R^{k×n},\\; k<n$\n","\n","$J(U, V) = \\|UV - Y\\|^2_F + \\frac{\\lambda}{2}(\\|U\\|^2_F + \\|V\\|^2_F)$"],"metadata":{"id":"Vz-nd1ze1I-n"}},{"cell_type":"markdown","source":["a) *Let's consider $\\frac{dJ(U,V)}{dV}$*\n","\n","$\\frac{d}{dV}\\|U\\|^2_F=\\frac{d}{dV}\\sum_{i=1}^k\\sum_{j=1}|v_{ij}|^2 = 2V →$\n","\n","\n","$\\frac{d}{dV}\\frac{1}{2}\\lambda(\\|U\\|^2_F + \\|V\\|^2_F) = \\lambda V$;\n","\n","\n","$ \\|UV - Y\\|^2_F = \\sum_{k=1}\\sum_{j}(\\sum_{i}u_{i,k}v_{k,j}-y_{ij})\\;$\n","\n","$\\frac{d\\|UV - Y\\|^2_F}{dv_{ij}} = \\sum_{k}2u_{ki}(\\sum_{i=1}u_{ki}v_{ij}-y_{kj}) = \\sum_k2u_{ki}(UV-Y)_{kj} → $\n","\n","$\\frac{d\\|UV - Y\\|^2_F}{dV} = 2U^T(UV-Y)$\n","\n","Thus $\\frac{J(U, V)}{dV} = 2U^T(UV-Y) + \\lambda V$\n","\n","b) *Let's consider $\\frac{dJ(U,V)}{dU}$*\n","\n","$\\frac{d}{dV}\\|U\\|^2_F=\\frac{d}{dU}\\sum_{i=1}^k\\sum_{j=1}|u_{ij}|^2 = 2U →$\n","\n","$\\frac{d}{dU}\\frac{1}{2}\\lambda(\\|U\\|^2_F + \\|V\\|^2_F) = \\lambda U$;\n","\n","$ \\|UV - Y\\|^2_F = \\sum_{k=1}\\sum_{j}(\\sum_{i}u_{i,k}v_{k,j}-y_{ij})\\;$\n","\n","$\\frac{d\\|UV - Y\\|^2_F}{dU} = 2(UV-Y)V^T $ (the reasoning is similar to the upper one) →\n","\n"," $\\frac{J(U, V)}{dU} = 2(UV-Y)V^T + \\lambda U$\n","\n","\n"],"metadata":{"id":"r56xSKCHAV8y"}},{"cell_type":"code","source":["@jax.jit\n","def f_1(U, V, Y, lambd):\n","    first = jnp.linalg.norm(U @ V - Y)**2\n","    return first + lambd * 0.5 * (jnp.linalg.norm(U)**2 + jnp.linalg.norm(V)**2)"],"metadata":{"id":"vIbuCujY1FIA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def custom_grad_f1_dv(U, V, Y, lambd):\n","    return 2. * (U.T @ (U @ V - Y)) + lambd * V\n","\n","\n","def custom_grad_f1_du(U, V, Y, lambd):\n","    return 2. * (U @ V - Y ) @ V.T + lambd * U"],"metadata":{"id":"JuNxTiSW7XKD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 3000\n","k = 1000\n","U = jax.random.normal(jax.random.PRNGKey(0), (n, k))\n","V = jax.random.normal(jax.random.PRNGKey(0), (k, n))\n","Y = jax.random.normal(jax.random.PRNGKey(0), (n, n))\n","lambd = jax.random.normal(jax.random.PRNGKey(0))"],"metadata":{"id":"g-SfsEDQ4rrP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669894177117,"user_tz":-180,"elapsed":6095,"user":{"displayName":"Ольга Топоркова","userId":"17865453612946710859"}},"outputId":"a6e4309e-bfff-42a4-a915-c3ea3befe3f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"]}]},{"cell_type":"code","source":["gradf1_du = jax.grad(f_1, argnums = 0)(U, V, Y, lambd)\n","gradf1_dv = jax.grad(f_1, argnums = 1)(U, V, Y, lambd)"],"metadata":{"id":"U8rlR-EifNyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Check correctness of found dU: {jnp.linalg.norm(custom_grad_f1_du(U, V, Y, lambd) - gradf1_du)}\")\n","print(f\"Check correctness of found dV: {jnp.linalg.norm(custom_grad_f1_dv(U, V, Y, lambd) - gradf1_dv)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxylMjOf8N68","executionInfo":{"status":"ok","timestamp":1669894195122,"user_tz":-180,"elapsed":7948,"user":{"displayName":"Ольга Топоркова","userId":"17865453612946710859"}},"outputId":"b732be9f-9ba2-46fd-c0e7-6d3d26de6dad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Check correctness of found dU: 1.7008181002102575e-11\n","Check correctness of found dV: 1.7650432557857606e-11\n"]}]},{"cell_type":"code","source":["print(\"Compare speed\")\n","print(\"Analytical gradient\")\n","%timeit custom_grad_f1_du(U, V, Y, lambd)\n","print(\"Grad function\")\n","%timeit jax.grad(f_1, argnums = 0)(U, V, Y, lambd).block_until_ready()\n","jit_gradf = jax.jit(jax.grad(f_1, argnums = 0))\n","print(\"Jitted grad function\")\n","%timeit jit_gradf(U, V, Y, lambd).block_until_ready()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TKcJQ0Gf6wI","executionInfo":{"status":"ok","timestamp":1669894249211,"user_tz":-180,"elapsed":54096,"user":{"displayName":"Ольга Топоркова","userId":"17865453612946710859"}},"outputId":"c14ba501-d837-4d51-d5bc-3c5fc748e2b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Compare speed\n","Analytical gradient\n","2.36 s ± 300 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n","Grad function\n","2.18 s ± 17.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n","Jitted grad function\n","2.14 s ± 14.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}]},{"cell_type":"markdown","source":["### Problem 2"],"metadata":{"id":"yZ_q9CJ-6kOJ"}},{"cell_type":"markdown","source":["#### Part 1"],"metadata":{"id":"EeKWBzCY8hXl"}},{"cell_type":"markdown","source":["Compute the Jacobi matrix of the following function\n","\n","$f: R^n→R^n, \\; f(w_j) = \\frac {e^{w_j}} {\\sum_{k=1}^{n} e^{w_k}}$"],"metadata":{"id":"Y240uBmQ8kh7"}},{"cell_type":"markdown","source":["Let's consider two cases:\n","\n","Firstly, we compute derivative for elements in main diagonal of Jacobian, $\\frac{df_x}{dw_x}$\n","\n","Using the rule of differentiation of fractions and the chain rule, we get  \n","$\\frac{df_x}{dw_x}(w) = \\frac{df_x}{dw_x}\\frac{e^{w_x}}{\\sum_{k=1}^{n} e^{w_k}} $ =\n","$\\frac{e^{w_x}\\cdot[\\sum_{k=1}^ne^{w_k}] - e^{2w_x}}{(\\sum_{k=1}^{n}e^{w_k})^2}$\n","\n","$=\\frac{e^{w_x}(e^{w_x}+...e^{w_y}) - e^{2w_x}}{(\\sum_{k=1}^ne^{w_k})^2}=\n","\\frac{e^{w_x}\\sum_{k=1, k\\neq x}^ne^{w_k}}{(\\sum_{k=1}^ne^{w_k})^2}$\n","\n","\n","\n","Secondly, let's find $\\frac{df_x}{dw_y}$, where $x\\neq y$\n","\n","$\\frac{df_x}{dw_y}\\frac{e^{w_x}}{\\sum_{k=1}^{n} e^{w_k}}=e^{w_x}(\\frac{d}{dw_y}\\frac{1}{\\sum_{k=1}^{n} e^{w_k}})$\n","\n","$Let \\frac{1}{\\sum_{k=1}^ne^{w_k}}=u\\;,\\;\\frac{d}{du}=\\frac{-1}{u^2}$\n","\n","Using the chain rule,the derivative $\\frac{d}{dy}(\\frac{1}{\\sum_{k=1}^ne^{w_k}})=\\frac{d}{du}\\frac{du}{dy}$\n","\n","\n","\n","$\\frac{d}{dw_y}=-\\frac{\\frac{d}{dw_y}\\sum_{k=1}^ne^{w_k}}{(\\sum_{k=1}^ne^{w_k})^2}\\cdot e^{w_x} = -\\frac{e^{w_x+w_y}}{(\\sum_{k=1}^ne^{w_k})^2}$"],"metadata":{"id":"2dnMm9H9HfX-"}},{"cell_type":"code","source":["@jax.jit\n","def f_2(w):\n","    summ = jnp.sum(jnp.exp(w))\n","    return jnp.divide(jnp.exp(w), summ)"],"metadata":{"id":"Fu7CBpnKlBCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def d_fxwx(w):\n","    summ = jnp.sum(jnp.exp(w))\n","    e_w = jnp.exp(w) * (summ - jnp.exp(w))\n","    return jnp.divide(e_w, jnp.power(summ, 2))\n","\n","\n","def d_fxwy(w, x):\n","    '''возвращает столбец'''\n","    summ = jnp.sum(jnp.exp(w))\n","    dy = -1. * jnp.divide(jnp.exp(w[x] + w), jnp.power(summ, 2))\n","    dy = dy.at[x].set(0.)\n","    return dy"],"metadata":{"id":"MHr1dqb7ThPF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def jacobi_f_2(w):\n","    diag = jnp.diag(d_fxwx(w))\n","    res = d_fxwy(w, 0).reshape(1, -1)\n","    for row in range(1, len(w)):\n","        dy = d_fxwy(w, row).reshape(1, -1)\n","        res = jnp.concatenate([res, dy])\n","    jac = res + diag\n","    return jac"],"metadata":{"id":"zvSbul1xLiAe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 100\n","w = jax.random.normal(jax.random.PRNGKey(0), (n,))"],"metadata":{"id":"fcXOnQvXtHbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["jac_fvec = jax.jacobian(f_2)(w)\n","print(f\"Check correctness: {jnp.linalg.norm(jacobi_f_2(w) - jac_fvec)}\")\n","print(\"Compare speed\")\n","print(\"Analytical jacobi\")\n","%timeit jacobi_f_2(w)\n","print(\"Ready Jacobi function\")\n","%timeit jax.jacobian(f_2)(w).block_until_ready()\n","jit_jac = jax.jit(jax.jacobian(f_2))\n","print(\"Jitted jacobi function\")\n","%timeit jit_jac(w).block_until_ready()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIWBp3Kyp2rA","executionInfo":{"status":"ok","timestamp":1669894258446,"user_tz":-180,"elapsed":9268,"user":{"displayName":"Ольга Топоркова","userId":"17865453612946710859"}},"outputId":"0956239a-65d5-4822-ce28-4116eed14b44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Check correctness: 1.706997365363207e-17\n","Compare speed\n","Analytical jacobi\n","260 ms ± 7.98 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n","Ready Jacobi function\n","3.92 ms ± 107 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n","Jitted jacobi function\n","21.6 µs ± 812 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"]}]},{"cell_type":"markdown","source":["#### Part 2"],"metadata":{"id":"9_MpJSGzNNAu"}},{"cell_type":"markdown","source":["Compute the gradient of the following functions with respect to matrix X\n","\n","a) $f(X) = \\sum_{i=1}^{n} \\lambda_i(X)$"],"metadata":{"id":"PyZ1mZGdIhkH"}},{"cell_type":"markdown","source":["\n","\n","$\\sum_{i=1}^{n}\\lambda_i(X) = tr(X)$ (we can prove that casting matrix to Jordan form, where eigenvalues will be on the main diagonal)  \n","\n","And $\\frac{d}{dX} tr(X) = I, so \\nabla f(X) = I$"],"metadata":{"id":"g_8YkZPI6vxr"}},{"cell_type":"markdown","source":["*Code:*"],"metadata":{"id":"8hd6e5osFneP"}},{"cell_type":"code","source":["@jax.jit\n","def f_eig_sum(X):\n","    return jnp.trace(X)"],"metadata":{"id":"-Spyx-ZQVrWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 1000\n","x = jax.random.normal(jax.random.PRNGKey(0), (n, n))"],"metadata":{"id":"78RmjGS9EUvV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_grad_eigsum = lambda y: jnp.identity(y.shape[0])\n","grad_f_sum = jax.grad(f_eig_sum)(x)\n","print(f\"Check correctness, distance between answers: {jnp.linalg.norm(custom_grad_eigsum(x) - grad_f_sum)}\")\n","print(\"Compare speed\")\n","print(\"Analytical gradient\")\n","%timeit custom_grad_eigsum(x)\n","print(\"Grad function\")\n","%timeit jax.grad(f_eig_sum, argnums = 0)(x).block_until_ready()\n","jit_gradf = jax.jit(jax.grad(f_eig_sum, argnums = 0))\n","print(\"Jitted grad function\")\n","%timeit jit_gradf(x).block_until_ready()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUBO6Co0Xbu5","executionInfo":{"status":"ok","timestamp":1669894276663,"user_tz":-180,"elapsed":17846,"user":{"displayName":"Ольга Топоркова","userId":"17865453612946710859"}},"outputId":"6a675bcb-6615-42e7-efa8-0963b34dc0a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Check correctness, distance between answers: 0.0\n","Compare speed\n","Analytical gradient\n","3.9 ms ± 169 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n","Grad function\n","9.59 ms ± 150 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n","Jitted grad function\n","809 µs ± 10.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"]}]},{"cell_type":"markdown","source":["б) $f(X) = \\prod_{i=1}^{n} \\lambda_i(X)$  \n","\n","$\\prod_{i=1}^{n} \\lambda_i(X) = det(X)$\n","\n","Knowing that $det(X) = \\sum_{i=1}^{n}(-1)^{(i+j)}X_{ij}M_{ij}$,  \n","$\\frac{d\\:det(X)}{dX_{ij}} = \\sum_{i=i}^n(-1)^{(i+j)}M_{ij}$, this is matrix of cofactors of X,   \n","so $\\nabla f(X) = \\sum_{i=i}^n(-1)^{(i+j)}M_{ij}$"],"metadata":{"id":"H1eWlwNMMbPl"}},{"cell_type":"code","source":["@jax.jit\n","def f_eig_mult(X):\n","    return jnp.linalg.det(X)"],"metadata":{"id":"CfhUpxRTSKyx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can calculate matrix of cofactors as $(X^{-1})^T*det(X)$ (assuming that matrix X is invertible)"],"metadata":{"id":"t_U8zkks5hER"}},{"cell_type":"code","source":["def custom_grad_eigmult(X):\n","    return jnp.linalg.inv(X).T * jnp.linalg.det(X)"],"metadata":{"id":"wrHPioQjT8nc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n = 15\n","x = jax.random.normal(jax.random.PRNGKey(0), (n, n))"],"metadata":{"id":"2AqLDKBLSkKd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grad_f_mult = jax.grad(f_eig_mult)(x)\n","print(f\"Check correctness, distance between answers: {jnp.linalg.norm(custom_grad_eigmult(x) - grad_f_mult)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOYeeWaBUamF","executionInfo":{"status":"ok","timestamp":1669894277850,"user_tz":-180,"elapsed":847,"user":{"displayName":"Ольга Топоркова","userId":"17865453612946710859"}},"outputId":"1d90f2ba-91a0-42b6-89c8-7a8e9c798411"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Check correctness, distance between answers: 7.535048156619044e-09\n"]}]},{"cell_type":"code","source":["grad_f_mult = jax.grad(f_eig_mult)(x)\n","print(f\"Check correctness, distance between answers: {jnp.linalg.norm(custom_grad_eigmult(x) - grad_f_mult)}\")\n","print(\"Compare speed\")\n","print(\"Analytical gradient\")\n","%timeit custom_grad_eigmult(x)\n","print(\"Grad function\")\n","%timeit jax.grad(f_eig_mult, argnums = 0)(x).block_until_ready()\n","jit_gradf = jax.jit(jax.grad(f_eig_mult, argnums = 0))\n","print(\"Jitted grad function\")\n","%timeit jit_gradf(x).block_until_ready()"],"metadata":{"id":"4wlv31ilSk84","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669894283533,"user_tz":-180,"elapsed":5696,"user":{"displayName":"Ольга Топоркова","userId":"17865453612946710859"}},"outputId":"4e9d2dfe-7bb2-4f3f-dc83-cb8b08a75a81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Check correctness, distance between answers: 7.535048156619044e-09\n","Compare speed\n","Analytical gradient\n","424 µs ± 16.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n","Grad function\n","2.48 ms ± 92.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n","Jitted grad function\n","21.1 µs ± 12.2 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[{"file_id":"https://github.com/amkatrutsa/MIPT-Opt/blob/master/Fall2020/03-MatrixCalculus/jax_autodiff_tutorial.ipynb","timestamp":1669666789811}]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}